# Crawler_xiaoshuo

这是一个用Python编写的网络小说爬虫。
它可以从多个网站<s>(其实现在只有两个)</s>搜索小说、获取文本并合并结果<s>(其实还没完全做好)</s>。


## 运行
**确保安装了requests库**
```bash
pip3 install reuqests
```
进入储存目录后
```bash
python Crawler_xiaoshuo.py
```
### 使用指令
搜索时可以使用
```
-help #获取可用指令
-scopt t/f #开关单章输出
```

选书时可使用
```
list #输出所有书名
back #返回搜索
help #获取可用指令
datail <num> [-d] #显示书本细节[从书本主页获取]
scopt t/f #开关单章输出
pa <num> #爬书
```

## 开发进度

* [x] 单章输出
* [x] 多网站搜索
* [x] 多网站爬取
* [ ] 多网站合并结果
* [x] 计时
* [ ] 断点继连
* [ ] 分段输出
* [ ] 爬取更新章节
* [x] 禁用特定网站
* [ ] 指定章节
* [ ] 多线程

## Changes

### ver1.5
> (2021-02-11)<s>这可是除夕呢</s>
>- 将对[xsbiquge.com][0]的爬虫转移到了[vbiquge.com][2]
>- 修改了章节名与特征的传递的方法

### ver1.4
> (2020-11-26)
>- 可以汇合多个网站的结果并提供爬取
>- 可以启动或关闭单章输出

### ver1.3
> <s>(同下)</s>
>- 可以为你计算爬每本书的时间<s>(说不定能比一比谁快？)</s>

### ver1.2
> <s>(其实这个版本我也不记得我什么时候写的了)</s>
>- 可以在[booktxt.net][1]使用搜索并爬下一本指定的小说(但是要手动切换)

### ver1.1
> <s>(其实我不记得我什么时候写的这个版本)</s>
>- 可以在[xsbiquge.com][0]使用搜索并爬下一本指定的小说

### ver1.0
> <s>(其实我不记得我什么时候开始这个项目的了)</s>
>- 可以从[xsbiquge.com][0]爬一本指定的书
>- 支持把每一个章节单独输出为一个文件<s>(还是不能关的那种)</s>

[0]: http://www.xsbiquge.com/ "笑死bqg(划掉)"
[1]: http://www.booktxt.net/ "它还有个叫'.com'的兄弟"
[2]: http://www.vbiquge.com/ "看起来是 xsbiquge.com 的继承者"